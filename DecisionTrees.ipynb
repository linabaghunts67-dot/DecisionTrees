{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XE_4UNMX0oE"
      },
      "source": [
        "#Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "gXMTT7SLWPcT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_classification\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, validation_curve\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree, export_text\n",
        "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import graphviz\n",
        "from collections import Counter\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBMDpVcygYMB"
      },
      "source": [
        "#DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9eTa6vvX4Y7",
        "outputId": "dd9d0d92-dcc4-4566-9373-fcf71a694348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Day   Outlook Temperature Humidity    Wind Play Tennis\n",
            "0     1     Sunny         Hot     High    Weak          No\n",
            "1     2     Sunny         Hot     High  Strong          No\n",
            "2     3  Overcast         Hot     High    Weak         Yes\n",
            "3     4      Rain        Mild     High    Weak         Yes\n",
            "4     5      Rain        Cool   Normal    Weak         Yes\n",
            "5     6      Rain        Cool   Normal  Strong          No\n",
            "6     7  Overcast        Cool   Normal  Strong         Yes\n",
            "7     8     Sunny        Mild     High    Weak          No\n",
            "8     9     Sunny        Cool   Normal    Weak         Yes\n",
            "9    10      Rain        Mild   Normal    Weak         Yes\n",
            "10   11     Sunny        Mild   Normal  Strong         Yes\n",
            "11   12  Overcast        Mild     High  Strong         Yes\n",
            "12   13  Overcast         Hot   Normal    Weak         Yes\n",
            "13   14      Rain        Mild     High  Strong          No\n"
          ]
        }
      ],
      "source": [
        "data = {\n",
        "    \"Day\": [1,2,3,4,5,6,7,8,9,10,11,12,13,14],\n",
        "    \"Outlook\": [\"Sunny\",\"Sunny\",\"Overcast\",\"Rain\",\"Rain\",\"Rain\",\"Overcast\",\"Sunny\",\"Sunny\",\"Rain\",\"Sunny\",\"Overcast\",\"Overcast\",\"Rain\"],\n",
        "    \"Temperature\": [\"Hot\",\"Hot\",\"Hot\",\"Mild\",\"Cool\",\"Cool\",\"Cool\",\"Mild\",\"Cool\",\"Mild\",\"Mild\",\"Mild\",\"Hot\",\"Mild\"],\n",
        "    \"Humidity\": [\"High\",\"High\",\"High\",\"High\",\"Normal\",\"Normal\",\"Normal\",\"High\",\"Normal\",\"Normal\",\"Normal\",\"High\",\"Normal\",\"High\"],\n",
        "    \"Wind\": [\"Weak\",\"Strong\",\"Weak\",\"Weak\",\"Weak\",\"Strong\",\"Strong\",\"Weak\",\"Weak\",\"Weak\",\"Strong\",\"Strong\",\"Weak\",\"Strong\"],\n",
        "    \"Play Tennis\": [\"No\",\"No\",\"Yes\",\"Yes\",\"Yes\",\"No\",\"Yes\",\"No\",\"Yes\",\"Yes\",\"Yes\",\"Yes\",\"Yes\",\"No\"]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgNkbF7AgnVt"
      },
      "source": [
        "#Calculating Entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssb3Te4UX4WC",
        "outputId": "5e79c1d7-4757-4495-b736-df275a6763ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9261634981262887\n"
          ]
        }
      ],
      "source": [
        "days = 14\n",
        "pYes = 9/days\n",
        "pNo = 4/days\n",
        "entropy = -pYes * math.log2(pYes) - pNo * math.log2(pNo)\n",
        "print(entropy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a9mQcqrryw1"
      },
      "source": [
        "#Info gain for each"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GsMVdSnX4Qf",
        "outputId": "90a0cb3d-3eea-4cbf-c480-9cebc40ec166"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outlook: 0.23262735923009692, temp: 0.015100105114612461, humidity: 0.13771304081799918, wind: 0.03400456986392708\n"
          ]
        }
      ],
      "source": [
        "p_yes_sunny, p_no_sunny = 2/5, 3/5\n",
        "p_yes_rain, p_no_rain = 3/5, 2/5\n",
        "\n",
        "ent_sunny = -p_yes_sunny * math.log2(p_yes_sunny) - p_no_sunny * math.log2(p_no_sunny)\n",
        "ent_overcast = 0\n",
        "ent_rain = -(p_yes_rain * math.log2(p_yes_rain)) - (p_no_rain * math.log2(p_no_rain))\n",
        "\n",
        "weighted_outlook = 5/14 * ent_sunny + 4/14 * ent_overcast + 5/14 * ent_rain\n",
        "inf_outlook = entropy - weighted_outlook\n",
        "\n",
        "\n",
        "p_yes_hot, p_no_hot = 2/4, 2/4\n",
        "p_yes_mild, p_no_mild = 4/6, 2/6\n",
        "p_yes_cool, p_no_cool = 3/4, 1/4\n",
        "\n",
        "ent_mild = -(p_yes_mild * math.log2(p_yes_mild)) - (p_no_mild * math.log2(p_no_mild))\n",
        "ent_cool = -(p_yes_cool * math.log2(p_yes_cool) + p_no_cool * math.log2(p_no_cool))\n",
        "ent_hot = -(p_yes_hot * math.log2(p_yes_hot)) - (p_no_hot * math.log2(p_no_hot))\n",
        "\n",
        "weighted_temp = (4/14) * ent_hot + (6/14) * ent_mild + (4/14) * ent_cool\n",
        "inf_temp = entropy - weighted_temp\n",
        "\n",
        "\n",
        "p_yes_high, p_no_high = 3/7, 4/7\n",
        "p_yes_normal, p_no_normal = 6/7, 1/7\n",
        "\n",
        "ent_high = -(p_yes_high*math.log2(p_yes_high) + p_no_high*math.log2(p_no_high))\n",
        "ent_nornal = -(p_yes_normal*math.log2(p_yes_normal) + p_no_normal*math.log2(p_no_normal))\n",
        "\n",
        "weighted_humidity = 7/14 * ent_high + 7/14 * ent_nornal\n",
        "inf_humidity = entropy - weighted_humidity\n",
        "\n",
        "\n",
        "ent_strong = 1\n",
        "p_weak_yes, p_weak_no = 6/8, 2/8\n",
        "ent_weak = -(p_weak_yes*math.log2(p_weak_yes) + p_weak_no*math.log2(p_weak_no))\n",
        "\n",
        "weighted_wind = 6/14 * ent_strong + 8/14 * ent_weak\n",
        "inf_wind = entropy - weighted_wind\n",
        "\n",
        "print(f\"Outlook: {inf_outlook}, temp: {inf_temp}, humidity: {inf_humidity}, wind: {inf_wind}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6qxWTxOr1wI"
      },
      "source": [
        "#Gini impurity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7JNJFyMX4Nf",
        "outputId": "8a095faa-0a3d-4fa7-cee2-22c023b2b272"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gini Outlook: 0.3429\n",
            "Gini Temperature: 0.4405\n",
            "Gini Humidity: 0.3673\n",
            "Gini Wind: 0.2857\n"
          ]
        }
      ],
      "source": [
        "def gini(p_yes, p_no):\n",
        "    return 1 - (p_yes**2 + p_no**2)\n",
        "\n",
        "p_yes_sunny, p_no_sunny = 2/5, 3/5\n",
        "p_yes_overcast, p_no_overcast = 4/4, 0/4\n",
        "p_yes_rain, p_no_rain = 3/5, 2/5\n",
        "gini_sunny = gini(p_yes_sunny, p_no_sunny)\n",
        "gini_overcast = gini(p_yes_overcast, p_no_overcast)\n",
        "gini_rain = gini(p_yes_rain, p_no_rain)\n",
        "weighted_gini_outlook = (5/14)*gini_sunny + (4/14)*gini_overcast + (5/14)*gini_rain\n",
        "print(f\"Gini Outlook: {weighted_gini_outlook:.4f}\")\n",
        "\n",
        "p_yes_hot, p_no_hot = 2/4, 2/4\n",
        "p_yes_mild, p_no_mild = 4/6, 2/6\n",
        "p_yes_cool, p_no_cool = 3/4, 1/4\n",
        "gini_hot = gini(p_yes_hot, p_no_hot)\n",
        "gini_mild = gini(p_yes_mild, p_no_mild)\n",
        "gini_cool = gini(p_yes_cool, p_no_cool)\n",
        "weighted_gini_temp = (4/14)*gini_hot + (6/14)*gini_mild + (4/14)*gini_cool\n",
        "print(f\"Gini Temperature: {weighted_gini_temp:.4f}\")\n",
        "\n",
        "p_yes_high, p_no_high = 3/7, 4/7\n",
        "p_yes_normal, p_no_normal = 6/7, 1/7\n",
        "gini_high = gini(p_yes_high, p_no_high)\n",
        "gini_normal = gini(p_yes_normal, p_no_normal)\n",
        "weighted_gini_humidity = (7/14)*gini_high + (7/14)*gini_normal\n",
        "print(f\"Gini Humidity: {weighted_gini_humidity:.4f}\")\n",
        "\n",
        "p_yes_strong, p_no_strong = 1/2, 1/2\n",
        "p_yes_weak, p_no_weak = 6/8, 2/8\n",
        "gini_strong = gini(p_yes_strong, p_no_strong)\n",
        "gini_weak = gini(p_yes_weak, p_no_weak)\n",
        "weighted_gini_wind = (2/14)*gini_strong + (8/14)*gini_weak\n",
        "print(f\"Gini Wind: {weighted_gini_wind:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpH57BJdr4cV"
      },
      "source": [
        "#Constructing Trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqzWsSn3X35w",
        "outputId": "bd0bb6f8-0fbc-477a-ef49-3812746e250d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node classes created successfully!\n",
            "Root node: Node(feature=None, threshold=None, samples=0)\n",
            "Leaf node: Leaf(prediction=None, samples=0)\n"
          ]
        }
      ],
      "source": [
        "class TreeNode:\n",
        "    \"\"\"\n",
        "    Represents a node in the decision tree.\n",
        "\n",
        "    Can be either an internal node with a splitting rule or a leaf node\n",
        "    with a prediction.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, is_leaf=False):\n",
        "\n",
        "        self.is_leaf = is_leaf\n",
        "\n",
        "        self.feature_index = None\n",
        "        self.threshold = None\n",
        "        self.feature_value = None\n",
        "\n",
        "        self.prediction = None\n",
        "        self.class_counts = None\n",
        "\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "        self.children = {}\n",
        "\n",
        "        self.samples = 0\n",
        "        self.impurity = 0.0\n",
        "        self.depth = 0\n",
        "\n",
        "    def __repr__(self):\n",
        "        if self.is_leaf:\n",
        "            return f\"Leaf(prediction={self.prediction}, samples={self.samples})\"\n",
        "        else:\n",
        "            return f\"Node(feature={self.feature_index}, threshold={self.threshold}, samples={self.samples})\"\n",
        "\n",
        "root = TreeNode()\n",
        "leaf = TreeNode(is_leaf=True)\n",
        "print(\"Node classes created successfully!\")\n",
        "print(f\"Root node: {root}\")\n",
        "print(f\"Leaf node: {leaf}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaQg07gUuQiz"
      },
      "source": [
        "#Impurity Measures Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lh6xflzzuQ_M",
        "outputId": "b99295e0-d0c0-4f40-aa41-05f3e75d2dc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing impurity measures:\n",
            "Pure set entropy: -0.0\n",
            "Mixed set entropy: 1.0\n",
            "Skewed set entropy: 0.8112781244591328\n",
            "\n",
            "Gini impurity: \n",
            "Gini for pure 0.0\n",
            "Gini for mixed 0.5\n",
            "Gini for skewed 0.375\n",
            "\n",
            "MSE:\n",
            "MSE for pure 0.0\n",
            "MSE for mixed 0.25\n",
            "MSE for skewed 0.1875\n"
          ]
        }
      ],
      "source": [
        "def calculate_entropy(y):\n",
        "    \"\"\"\n",
        "    Calculate the entropy of a target variable.\n",
        "\n",
        "    Entropy is a measure of the impurity or randomness of a set of labels.\n",
        "    A set with perfect purity (all labels are the same) has entropy 0.\n",
        "    A set with maximum impurity (labels are equally distributed) has higher entropy.\n",
        "\n",
        "    Parameters:\n",
        "    y: array-like, target labels\n",
        "\n",
        "    Returns:\n",
        "    float: calculated entropy\n",
        "    \"\"\"\n",
        "    values, counts = np.unique(y, return_counts = True)\n",
        "    probs = counts / counts.sum()\n",
        "    return -np.sum(probs * np.log2(probs))\n",
        "\n",
        "def calculate_gini_impurity(y):\n",
        "    \"\"\"\n",
        "    Calculate the Gini impurity of a target variable.\n",
        "\n",
        "    Gini impurity is another measure of the impurity or randomness of a set of labels.\n",
        "    It represents the probability of incorrectly classifying a randomly chosen element\n",
        "    in the dataset if it were randomly labeled according to the distribution of labels\n",
        "    in the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    y: array-like, target labels\n",
        "\n",
        "    Returns:\n",
        "    float: calculated Gini impurity\n",
        "    \"\"\"\n",
        "    values, counts = np.unique(y, return_counts=True)\n",
        "    probs = counts / counts.sum()\n",
        "    return 1 - np.sum(probs**2)\n",
        "\n",
        "def calculate_mse(y, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the Mean Squared Error (MSE).\n",
        "\n",
        "    MSE is a common metric for evaluating the performance of regression models.\n",
        "    In the context of decision trees, it's used as an impurity measure for regression trees,\n",
        "    representing the variance of the target values in a node.\n",
        "\n",
        "    Parameters:\n",
        "    y: array-like, true target values\n",
        "    y_pred: array-like, predicted target values (can be the mean of y for a node)\n",
        "\n",
        "    Returns:\n",
        "    float: calculated MSE\n",
        "    \"\"\"\n",
        "    mean_y = np.mean(y)\n",
        "    return np.mean((y - mean_y) ** 2)\n",
        "\n",
        "test_labels_pure = np.array([1, 1, 1, 1])\n",
        "test_labels_mixed = np.array([1, 0, 1, 0])\n",
        "test_labels_skewed = np.array([1, 1, 1, 0])\n",
        "\n",
        "print(\"Testing impurity measures:\")\n",
        "print(f\"Pure set entropy: {calculate_entropy(test_labels_pure)}\")\n",
        "print(f\"Mixed set entropy: {calculate_entropy(test_labels_mixed)}\")\n",
        "print(f\"Skewed set entropy: {calculate_entropy(test_labels_skewed)}\")\n",
        "\n",
        "print(\"\\nGini impurity: \")\n",
        "print(\"Gini for pure\", calculate_gini_impurity(test_labels_pure))\n",
        "print(\"Gini for mixed\", calculate_gini_impurity(test_labels_mixed))\n",
        "print(\"Gini for skewed\", calculate_gini_impurity(test_labels_skewed))\n",
        "\n",
        "print(\"\\nMSE:\")\n",
        "print(\"MSE for pure\", calculate_mse(test_labels_pure, test_labels_pure))\n",
        "print(\"MSE for mixed\", calculate_mse(test_labels_mixed, test_labels_mixed))\n",
        "print(\"MSE for skewed\", calculate_mse(test_labels_skewed, test_labels_skewed))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrbCo5vQ11gv"
      },
      "source": [
        "#Feature Splitting Logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBoN0Qp4uRn5",
        "outputId": "a5d34bb7-c061-4dd0-b900-5f734cbe6c01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing split finding...\n",
            "Categorical split:\n",
            "(np.float64(3.0), np.float64(0.17095059445466854), array([3]), array([0, 1, 2, 4]))\n",
            "Continuous split:\n",
            "(np.float64(2.7), np.float64(0.4199730940219749), array([1, 3, 0]), array([4, 2]))\n"
          ]
        }
      ],
      "source": [
        "def find_best_split_categorical(X_column, y, impurity_func):\n",
        "    \"\"\"\n",
        "    Find the best split for a categorical feature.\n",
        "\n",
        "    For categorical features, we try splitting on each possible value,\n",
        "    creating one branch for that value and another for all other values.\n",
        "\n",
        "    Parameters:\n",
        "    X_column: array-like, values of a single categorical feature\n",
        "    y: array-like, target labels\n",
        "    impurity_func: function, impurity measure to use\n",
        "\n",
        "    Returns:\n",
        "    best_value: value to split on\n",
        "    best_impurity_reduction: improvement in impurity\n",
        "    left_indices: indices of samples going to left child\n",
        "    right_indices: indices of samples going to right child\n",
        "    \"\"\"\n",
        "    uniques = np.unique(X_column)\n",
        "    best_value = None\n",
        "    best_reduction = -1\n",
        "    best_left_indices = None\n",
        "    best_right_indices = None\n",
        "\n",
        "    parent_impurity = impurity_func(y)\n",
        "\n",
        "    for value in uniques:\n",
        "        left_indices = np.where(X_column == value)[0]\n",
        "        right_indices = np.where(X_column != value)[0]\n",
        "\n",
        "        if len(left_indices) == 0 or len(right_indices) == 0:\n",
        "            continue\n",
        "\n",
        "        left_labels = y[left_indices]\n",
        "        right_labels = y[right_indices]\n",
        "\n",
        "        left_impurity = impurity_func(left_labels)\n",
        "        right_impurity = impurity_func(right_labels)\n",
        "\n",
        "        weighted_impurity = (len(left_labels)/len(y)) * left_impurity + (len(right_labels)/len(y)) * right_impurity\n",
        "        reduction = parent_impurity - weighted_impurity\n",
        "\n",
        "        if reduction > best_reduction:\n",
        "            best_value = value\n",
        "            best_reduction = reduction\n",
        "            best_left_indices = left_indices\n",
        "            best_right_indices = right_indices\n",
        "\n",
        "    return best_value, best_reduction, best_left_indices, best_right_indices\n",
        "\n",
        "\n",
        "def find_best_split_continuous(X_column, y, impurity_func):\n",
        "    \"\"\"\n",
        "    Find the best split threshold for a continuous feature.\n",
        "\n",
        "    We consider all possible thresholds (typically midpoints between\n",
        "    consecutive sorted values) and choose the one that maximizes\n",
        "    information gain.\n",
        "\n",
        "    Parameters:\n",
        "    X_column: array-like, values of a single continuous feature\n",
        "    y: array-like, target labels\n",
        "    impurity_func: function, impurity measure\n",
        "\n",
        "    Returns:\n",
        "    best_threshold: threshold value for split\n",
        "    best_impurity_reduction: improvement in impurity\n",
        "    left_indices: indices of samples with value <= threshold\n",
        "    right_indices: indices of samples with value > threshold\n",
        "    \"\"\"\n",
        "    X_sorted = np.sort(X_column)\n",
        "    sorted_indices = np.argsort(X_column)\n",
        "    y_sorted = y[sorted_indices]\n",
        "\n",
        "    best_threshold = None\n",
        "    best_reduction = -1\n",
        "    best_left_indices = None\n",
        "    best_right_indices = None\n",
        "\n",
        "    parent_impurity = impurity_func(y)\n",
        "\n",
        "    for i in range(1, len(X_column)):\n",
        "        threshold = (X_sorted[i-1] + X_sorted[i]) / 2\n",
        "        left_indices = sorted_indices[np.where(X_column[sorted_indices] <= threshold)[0]]\n",
        "        right_indices = sorted_indices[np.where(X_column[sorted_indices] > threshold)[0]]\n",
        "\n",
        "        if len(left_indices) == 0 or len(right_indices) == 0:\n",
        "            continue\n",
        "\n",
        "        left_labels = y[left_indices]\n",
        "        right_labels = y[right_indices]\n",
        "\n",
        "        left_impurity = impurity_func(left_labels)\n",
        "        right_impurity = impurity_func(right_labels)\n",
        "\n",
        "        weighted_impurity = (len(left_labels)/len(y)) * left_impurity + (len(right_labels)/len(y)) * right_impurity\n",
        "        reduction = parent_impurity - weighted_impurity\n",
        "\n",
        "        if reduction > best_reduction:\n",
        "            best_threshold = threshold\n",
        "            best_reduction = reduction\n",
        "            best_left_indices = left_indices\n",
        "            best_right_indices = right_indices\n",
        "\n",
        "    return best_threshold, best_reduction, best_left_indices, best_right_indices\n",
        "\n",
        "def variance(y):\n",
        "    return np.var(y)\n",
        "\n",
        "def find_best_feature_split(X, y, feature_types, impurity_func):\n",
        "    \"\"\"\n",
        "    Find the best feature and split point across all features.\n",
        "\n",
        "    This function orchestrates the search across all possible features\n",
        "    and returns the overall best split for building the tree.\n",
        "\n",
        "    Parameters:\n",
        "    X: array-like, shape (n_samples, n_features), feature matrix\n",
        "    y: array-like, target values\n",
        "    feature_types: list, 'categorical' or 'continuous' for each feature\n",
        "    impurity_func: function, impurity measure\n",
        "\n",
        "    Returns:\n",
        "    best_feature: index of best feature to split on\n",
        "    best_split_info: dictionary with split details\n",
        "    \"\"\"\n",
        "    best_gain = -1\n",
        "    best_feature = None\n",
        "    best_split = None\n",
        "    current_impurity = impurity_func(y)\n",
        "    n_samples, n_features = X.shape\n",
        "    for feature_index in range(n_features):\n",
        "        if feature_types[feature_index] == 'continuous':\n",
        "            best_threshold, gain, left_mask, right_mask = find_best_split_continuous(X[:, feature_index], y, impurity_func)\n",
        "            if gain is not None and gain > best_gain:\n",
        "                best_gain = gain\n",
        "                best_feature = feature_index\n",
        "                best_split = {'type':'continuous','threshold':best_threshold,'left':left_mask,'right':right_mask}\n",
        "        else:\n",
        "            best_value, gain, left_mask, right_mask = find_best_split_categorical(X[:, feature_index], y, impurity_func)\n",
        "            if gain is not None and gain > best_gain:\n",
        "                best_gain = gain\n",
        "                best_feature = feature_index\n",
        "                best_split = {'type':'categorical','value':best_value,'left':left_mask,'right':right_mask}\n",
        "    return best_feature, best_split\n",
        "\n",
        "\n",
        "sample_X = np.array([[1, 2.5], [2, 1.8], [1, 3.2], [3, 2.1], [2, 2.9]])\n",
        "sample_y = np.array([0, 0, 1, 1, 1])\n",
        "feature_types = ['categorical', 'continuous']\n",
        "\n",
        "print(\"Testing split finding...\")\n",
        "\n",
        "print(\"Categorical split:\")\n",
        "print(find_best_split_categorical(sample_X[:, 0], sample_y, calculate_entropy))\n",
        "\n",
        "print(\"Continuous split:\")\n",
        "print(find_best_split_continuous(sample_X[:, 1], sample_y, calculate_entropy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvXD5mbRePho"
      },
      "source": [
        "#Tree Building Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9MKEgLKuRhr",
        "outputId": "aeb8b325-3341-43f5-d42b-31e6ac17bb8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 1 1]\n"
          ]
        }
      ],
      "source": [
        "class DecisionTreeClassifier:\n",
        "\n",
        "    \"\"\"\n",
        "    Custom implementation of Decision Tree Classifier.\n",
        "\n",
        "    This implementation provides full control over the tree building process\n",
        "    and helps you understand every decision made during construction.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1,\n",
        "                 impurity_measure='entropy', feature_types=None):\n",
        "\n",
        "        \"\"\"\n",
        "        Initialize the decision tree classifier.\n",
        "\n",
        "        Parameters:\n",
        "        max_depth: int or None, maximum depth of the tree\n",
        "        min_samples_split: int, minimum samples required to split a node\n",
        "        min_samples_leaf: int, minimum samples required at a leaf node\n",
        "        impurity_measure: str, 'entropy' or 'gini'\n",
        "        feature_types: list, type of each feature ('categorical' or 'continuous')\n",
        "        \"\"\"\n",
        "\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.feature_types = feature_types\n",
        "        self.root = None\n",
        "        if impurity_measure == 'entropy':\n",
        "            self.impurity_func = calculate_entropy\n",
        "        elif impurity_measure == 'gini':\n",
        "            self.impurity_func = calculate_gini_impurity\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown impurity measure: {impurity_measure}\")\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "\n",
        "        \"\"\"\n",
        "        Recursively build the decision tree.\n",
        "\n",
        "        This is the core recursive algorithm that creates the tree structure\n",
        "        by repeatedly finding the best splits and creating child nodes.\n",
        "\n",
        "        Parameters:\n",
        "        X: feature matrix for current subset\n",
        "        y: target values for current subset\n",
        "        depth: current depth in the tree\n",
        "        node_indices: indices of samples at this node\n",
        "\n",
        "        Returns:\n",
        "        TreeNode: the constructed node (internal or leaf)\n",
        "        \"\"\"\n",
        "\n",
        "        if len(np.unique(y)) == 1:\n",
        "            leaf = TreeNode(is_leaf=True)\n",
        "            leaf.prediction = np.unique(y)[0]\n",
        "            leaf.samples = len(y)\n",
        "            leaf.depth = depth\n",
        "            return leaf\n",
        "        if self.max_depth is not None and depth >= self.max_depth:\n",
        "            leaf = TreeNode(is_leaf=True)\n",
        "            leaf.prediction = np.bincount(y).argmax()\n",
        "            leaf.samples = len(y)\n",
        "            leaf.depth = depth\n",
        "            return leaf\n",
        "        if len(y) < self.min_samples_split:\n",
        "            leaf = TreeNode(is_leaf=True)\n",
        "            leaf.prediction = np.bincount(y).argmax()\n",
        "            leaf.samples = len(y)\n",
        "            leaf.depth = depth\n",
        "            return leaf\n",
        "\n",
        "        best_feature, best_split = find_best_feature_split(X, y, self.feature_types, self.impurity_func)\n",
        "\n",
        "        if best_feature is None or len(best_split['left']) < self.min_samples_leaf or len(best_split['right']) < self.min_samples_leaf:\n",
        "            leaf = TreeNode(is_leaf=True)\n",
        "            leaf.prediction = np.bincount(y).argmax()\n",
        "            leaf.samples = len(y)\n",
        "            leaf.depth = depth\n",
        "            return leaf\n",
        "\n",
        "        node = TreeNode(is_leaf=False)\n",
        "        node.feature_index = best_feature\n",
        "        node.samples = len(y)\n",
        "        node.depth = depth\n",
        "\n",
        "        left_X, left_y = X[best_split['left']], y[best_split['left']]\n",
        "        right_X, right_y = X[best_split['right']], y[best_split['right']]\n",
        "\n",
        "        if best_split['type'] == 'categorical':\n",
        "            node.feature_value = best_split['value']\n",
        "        else:\n",
        "            node.threshold = best_split['threshold']\n",
        "\n",
        "        node.left = self._build_tree(left_X, left_y, depth + 1)\n",
        "        node.right = self._build_tree(right_X, right_y, depth + 1)\n",
        "\n",
        "        return node\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        \"\"\"\n",
        "        Build the decision tree from training data.\n",
        "\n",
        "        Parameters:\n",
        "        X: array-like, shape (n_samples, n_features), training features\n",
        "        y: array-like, shape (n_samples,), training labels\n",
        "        \"\"\"\n",
        "\n",
        "        if self.feature_types is None and X.shape[1] > 0:\n",
        "             raise ValueError(\"feature_types must be provided if X has multiple columns.\")\n",
        "\n",
        "\n",
        "        self.root = self._build_tree(X, y)\n",
        "        return self\n",
        "\n",
        "    def _predict_sample(self, x, node):\n",
        "\n",
        "        \"\"\"\n",
        "        Predict the class for a single sample by traversing the tree.\n",
        "\n",
        "        Parameters:\n",
        "        x: single sample (feature vector)\n",
        "        node: current node in traversal\n",
        "\n",
        "        Returns:\n",
        "        prediction: predicted class label\n",
        "        \"\"\"\n",
        "\n",
        "        if node.is_leaf:\n",
        "            return node.prediction\n",
        "        if self.feature_types[node.feature_index] == 'categorical':\n",
        "            if x[node.feature_index] == node.feature_value:\n",
        "                return self._predict_sample(x, node.left)\n",
        "            else:\n",
        "                return self._predict_sample(x, node.right)\n",
        "        else:\n",
        "            if x[node.feature_index] <= node.threshold:\n",
        "                return self._predict_sample(x, node.left)\n",
        "            else:\n",
        "                return self._predict_sample(x, node.right)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict classes for multiple samples.\n",
        "\n",
        "        Parameters:\n",
        "        X: array-like, shape (n_samples, n_features), test features\n",
        "\n",
        "        Returns:\n",
        "        predictions: array of predicted class labels\n",
        "        \"\"\"\n",
        "        predictions = []\n",
        "        for i in range(len(X)):\n",
        "            predictions.append(self._predict_sample(X[i], self.root))\n",
        "        return np.array(predictions)\n",
        "\n",
        "X = np.array([\n",
        "    [1, 2.5],\n",
        "    [1, 3.0],\n",
        "    [2, 1.8],\n",
        "    [2, 2.0]\n",
        "])\n",
        "y = np.array([0, 0, 1, 1])\n",
        "feature_types = ['categorical', 'continuous']\n",
        "\n",
        "tree = DecisionTreeClassifier(max_depth=2, feature_types=feature_types, impurity_measure='entropy', min_samples_leaf=1)\n",
        "tree.fit(X, y)\n",
        "predictions = tree.predict(X)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lY3LwdYeX-B"
      },
      "source": [
        "#Tree Visualization and Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "lJIvPAyVeYZf"
      },
      "outputs": [],
      "source": [
        "def print_tree(node, feature_names=None, class_names=None, indent=\"\", max_depth=10):\n",
        "    \"\"\"\n",
        "    Print a text representation of the decision tree.\n",
        "\n",
        "    This function creates a human-readable representation that shows\n",
        "    the decision rules learned by the tree.\n",
        "\n",
        "    Parameters:\n",
        "    node: TreeNode, root of tree/subtree to print\n",
        "    feature_names: list, names of features for readable output\n",
        "    class_names: list, names of classes for readable output\n",
        "    indent: str, current indentation level\n",
        "    max_depth: int, maximum depth to print (prevents overly long output)\n",
        "    \"\"\"\n",
        "    if node is None or max_depth == 0:\n",
        "        return None\n",
        "\n",
        "    if node.is_leaf:\n",
        "        prediction_text = class_names[node.prediction] if class_names and node.prediction is not None and isinstance(node.prediction, int) and node.prediction < len(class_names) and node.prediction >= 0 else str(node.prediction)\n",
        "        print(f\"{indent}Leaf: Prediction={prediction_text}, Samples={node.samples}\")\n",
        "\n",
        "    else:\n",
        "        feature_name = feature_names[node.feature_index] if feature_names and node.feature_index is not None else f\"feature[{node.feature_index}]\"\n",
        "        if node.feature_value is not None:\n",
        "            print(f\"{indent}Node: {feature_name} == {node.feature_value}? Samples={node.samples}\")\n",
        "            print_tree(node.left, feature_names, class_names, indent + \"  \", max_depth - 1)\n",
        "            print_tree(node.right, feature_names, class_names, indent + \"  \", max_depth - 1)\n",
        "        elif node.threshold is not None:\n",
        "            print(f\"{indent}Node: {feature_name} <= {node.threshold}? Samples={node.samples}\")\n",
        "            print_tree(node.left, feature_names, class_names, indent + \"  \", max_depth - 1)\n",
        "            print_tree(node.right, feature_names, class_names, indent + \"  \", max_depth - 1)\n",
        "        else:\n",
        "             print(f\"{indent}Node: Split condition not defined? Samples={node.samples}\")\n",
        "\n",
        "\n",
        "def visualize_tree_structure(node, feature_names=None, class_names=None, ax=None, x=0.5, y=1.0, dx=0.2, dy=0.15):\n",
        "    \"\"\"\n",
        "    Create a visual representation of the tree structure using Matplotlib.\n",
        "\n",
        "    Generate a matplotlib-based visualization showing the tree structure,\n",
        "    decision boundaries, and class distributions at each node.\n",
        "\n",
        "    Parameters:\n",
        "    node: TreeNode, root of tree/subtree to visualize\n",
        "    feature_names: list, names of features for readable output\n",
        "    class_names: list, names of classes for readable output\n",
        "    ax: matplotlib Axes object (optional, for drawing on existing axes)\n",
        "    x, y: coordinates for the current node\n",
        "    dx, dy: spacing parameters for child nodes\n",
        "    \"\"\"\n",
        "\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(12, 8))\n",
        "        ax.set_axis_off()\n",
        "\n",
        "\n",
        "    if node.is_leaf:\n",
        "        prediction_text = class_names[node.prediction] if class_names and node.prediction is not None and isinstance(node.prediction, int) and node.prediction < len(class_names) and node.prediction >= 0 else str(node.prediction)\n",
        "        text = f\"Predict: {prediction_text}\\nSamples: {node.samples}\"\n",
        "        ax.text(x, y, text, ha='center', va='center',\n",
        "                bbox=dict(boxstyle=\"round\", facecolor=\"lightblue\", ec=\"black\"))\n",
        "        return\n",
        "\n",
        "    feature_name = feature_names[node.feature_index] if feature_names and node.feature_index is not None else f\"feature[{node.feature_index}]\"\n",
        "    if node.feature_value is not None:\n",
        "        text = f\"{feature_name} == {node.feature_value}?\\nSamples: {node.samples}\"\n",
        "    elif node.threshold is not None:\n",
        "        text = f\"{feature_name} <= {node.threshold:.2f}?\\nSamples: {node.samples}\"\n",
        "    else:\n",
        "        text = f\"Split condition not defined?\\nSamples: {node.samples}\"\n",
        "\n",
        "    ax.text(x, y, text, ha='center', va='center', bbox=dict(boxstyle=\"round\", facecolor=\"lightgreen\", ec=\"black\"))\n",
        "\n",
        "    if node.left:\n",
        "        child_x = x - dx\n",
        "        child_y = y - dy\n",
        "        ax.plot([x, child_x], [y - 0.05, child_y + 0.05], 'k-')\n",
        "        visualize_tree_structure(node.left, feature_names, class_names, ax, child_x, child_y, dx / 2, dy)\n",
        "\n",
        "    if node.right:\n",
        "        child_x = x + dx\n",
        "        child_y = y - dy\n",
        "        ax.plot([x, child_x], [y - 0.05, child_y + 0.05], 'k-')\n",
        "        visualize_tree_structure(node.right, feature_names, class_names, ax, child_x, child_y, dx / 2, dy)\n",
        "\n",
        "\n",
        "def extract_decision_rules(node, feature_names=None, path=\"\", rules_list=None):\n",
        "    \"\"\"\n",
        "    Extract all decision rules from the tree as text.\n",
        "\n",
        "    Convert the tree into a set of if-then rules that can be easily\n",
        "    understood and potentially used outside the tree structure.\n",
        "\n",
        "    Parameters:\n",
        "    node: TreeNode, current node\n",
        "    feature_names: list, feature names\n",
        "    path: str, current path conditions\n",
        "    rules_list: list, accumulator for rules\n",
        "\n",
        "    Returns:\n",
        "    list: all decision rules as strings\n",
        "    \"\"\"\n",
        "\n",
        "    if rules_list is None:\n",
        "        rules_list = []\n",
        "\n",
        "    if node is None:\n",
        "        return rules_list\n",
        "\n",
        "    if node.is_leaf:\n",
        "        rules_list.append(f\"{path.strip(' AND ')} -> prediction: {node.prediction}\")\n",
        "        return rules_list\n",
        "\n",
        "    if node.feature_index is not None:\n",
        "        feature = feature_names[node.feature_index] if feature_names else f\"feature[{node.feature_index}]\"\n",
        "        if node.feature_value is not None:\n",
        "            extract_decision_rules(node.left, feature_names, path + f\"{feature} == {node.feature_value} AND \", rules_list)\n",
        "            extract_decision_rules(node.right, feature_names, path + f\"{feature} != {node.feature_value} AND \", rules_list)\n",
        "        elif node.threshold is not None:\n",
        "            extract_decision_rules(node.left, feature_names, path + f\"{feature} <= {node.threshold:.2f} AND \", rules_list)\n",
        "            extract_decision_rules(node.right, feature_names, path + f\"{feature} > {node.threshold:.2f} AND \", rules_list)\n",
        "        else:\n",
        "             # Handle cases where split condition is undefined\n",
        "             extract_decision_rules(node.left, feature_names, path, rules_list)\n",
        "             extract_decision_rules(node.right, feature_names, path, rules_list)\n",
        "\n",
        "\n",
        "    return rules_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdMj4DRpeY13"
      },
      "source": [
        "#Regression Tree Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGI6IJvNeZMb",
        "outputId": "d43cd4c9-0e09-4420-b27b-7b8a5ee13d1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing regression tree...\n",
            "[11. 11. 21. 21.]\n"
          ]
        }
      ],
      "source": [
        "class DecisionTreeRegressor:\n",
        "    \"\"\"\n",
        "    Custom implementation of Decision Tree Regressor.\n",
        "\n",
        "    Regression trees predict continuous values rather than discrete classes.\n",
        "    The main differences are in the impurity measure (MSE instead of entropy)\n",
        "    and prediction method (mean instead of majority vote).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n",
        "        \"\"\"\n",
        "        Initialize the decision tree regressor.\n",
        "        \"\"\"\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.root = None\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        \"\"\"\n",
        "        Build regression tree using MSE as splitting criterion.\n",
        "\n",
        "        Key differences from classification:\n",
        "        - Use MSE to measure impurity\n",
        "        - Leaf predictions are mean of target values\n",
        "        - Consider variance reduction instead of information gain\n",
        "        \"\"\"\n",
        "        if len(y) <= self.min_samples_split or (self.max_depth and depth >= self.max_depth):\n",
        "            leaf = TreeNode(is_leaf=True)\n",
        "            leaf.prediction = y.mean()\n",
        "            return leaf\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "        best_feature, best_threshold, best_mse = None, None, float('inf')\n",
        "\n",
        "        for feature in range(n_features):\n",
        "            thresholds = np.unique(X[:, feature])\n",
        "            for t in thresholds:\n",
        "                left_mask = X[:, feature] <= t\n",
        "                right_mask = ~left_mask\n",
        "                if left_mask.sum() < self.min_samples_leaf or right_mask.sum() < self.min_samples_leaf:\n",
        "                    continue\n",
        "                mse_split = (np.var(y[left_mask]) * left_mask.sum() +\n",
        "                             np.var(y[right_mask]) * right_mask.sum()) / n_samples\n",
        "                if mse_split < best_mse:\n",
        "                    best_mse = mse_split\n",
        "                    best_feature = feature\n",
        "                    best_threshold = t\n",
        "\n",
        "        if best_feature is None:\n",
        "            leaf = TreeNode(is_leaf=True)\n",
        "            leaf.prediction = y.mean()\n",
        "            return leaf\n",
        "\n",
        "        node = TreeNode(is_leaf=False)\n",
        "        node.feature_index = best_feature\n",
        "        node.threshold = best_threshold\n",
        "        left_mask = X[:, best_feature] <= best_threshold\n",
        "        right_mask = ~left_mask\n",
        "        node.left = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
        "        node.right = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
        "        return node\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Train the regression tree.\"\"\"\n",
        "        self.root = self._build_tree(np.array(X), np.array(y))\n",
        "\n",
        "    def _predict_sample(self, x, node):\n",
        "        if node.is_leaf:\n",
        "            return node.prediction\n",
        "        if x[node.feature_index] <= node.threshold:\n",
        "            return self._predict_sample(x, node.left)\n",
        "        else:\n",
        "            return self._predict_sample(x, node.right)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict_sample(x, self.root) for x in X])\n",
        "\n",
        "print(\"Testing regression tree...\")\n",
        "X = np.array([[1,2],[1,3],[2,1],[2,2]])\n",
        "y = np.array([10, 12, 20, 22])\n",
        "tree = DecisionTreeRegressor(max_depth=2)\n",
        "tree.fit(X, y)\n",
        "preds = tree.predict(X)\n",
        "print(preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRu0P_YE5EZ2"
      },
      "source": [
        "#Advanced Features and Optimizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1_-l6v05E1c",
        "outputId": "f39587d5-dc6a-4cd1-d366-e729d2b60b4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing advanced features...\n",
            "Feature importances: [0. 0.]\n",
            "Processed X:\n",
            " [[1 2]\n",
            " [1 3]\n",
            " [2 1]\n",
            " [2 2]]\n"
          ]
        }
      ],
      "source": [
        "def implement_pruning(tree_root, X_val, y_val, pruning_method='reduced_error'):\n",
        "    \"\"\"\n",
        "    Implement tree pruning to reduce overfitting.\n",
        "\n",
        "    Pruning removes subtrees that don't improve performance on validation data.\n",
        "    This is crucial for creating trees that generalize well to new data.\n",
        "\n",
        "    Parameters:\n",
        "    tree_root: TreeNode, root of tree to prune\n",
        "    X_val: validation features\n",
        "    y_val: validation labels\n",
        "    pruning_method: str, pruning strategy to use\n",
        "\n",
        "    Returns:\n",
        "    TreeNode: pruned tree root\n",
        "    \"\"\"\n",
        "    if tree_root is None or tree_root.is_leaf:\n",
        "        return tree_root\n",
        "\n",
        "    tree_root.left = implement_pruning(tree_root.left, X_val, y_val, pruning_method)\n",
        "    tree_root.right = implement_pruning(tree_root.right, X_val, y_val, pruning_method)\n",
        "\n",
        "    if tree_root.left and tree_root.left.is_leaf and tree_root.right and tree_root.right.is_leaf:\n",
        "        y_pred = []\n",
        "        for xi in X_val:\n",
        "            if tree_root.feature_value is not None:\n",
        "                y_pred.append(tree_root.left.prediction if xi[tree_root.feature_index] == tree_root.feature_value else tree_root.right.prediction)\n",
        "            else:\n",
        "                y_pred.append(tree_root.left.prediction if xi[tree_root.feature_index] <= tree_root.threshold else tree_root.right.prediction)\n",
        "        acc_with_subtree = np.mean(np.array(y_pred) == y_val)\n",
        "\n",
        "        leaf_prediction = np.round(np.mean([tree_root.left.prediction, tree_root.right.prediction]))\n",
        "        acc_as_leaf = np.mean(np.full_like(y_val, leaf_prediction) == y_val)\n",
        "\n",
        "        if acc_as_leaf >= acc_with_subtree:\n",
        "            leaf = TreeNode(is_leaf=True)\n",
        "            leaf.prediction = leaf_prediction\n",
        "            return leaf\n",
        "    return tree_root\n",
        "\n",
        "\n",
        "def implement_feature_importance(tree_root, n_features):\n",
        "    \"\"\"\n",
        "    Calculate feature importance scores based on tree structure.\n",
        "\n",
        "    Feature importance measures how much each feature contributes\n",
        "    to decreasing impurity across all splits in the tree.\n",
        "\n",
        "    Parameters:\n",
        "    tree_root: TreeNode, root of trained tree\n",
        "    n_features: int, number of features\n",
        "\n",
        "    Returns:\n",
        "    array: importance scores for each feature\n",
        "    \"\"\"\n",
        "    importance = np.zeros(n_features)\n",
        "    def traverse(node):\n",
        "        if node is None or node.is_leaf:\n",
        "            return\n",
        "        importance[node.feature_index] += 1\n",
        "        traverse(node.left)\n",
        "        traverse(node.right)\n",
        "    traverse(tree_root)\n",
        "    if np.sum(importance) > 0:\n",
        "        importance = importance / np.sum(importance)\n",
        "    return importance\n",
        "\n",
        "def implement_missing_value_handling(X, method='surrogate'):\n",
        "    \"\"\"\n",
        "    Handle missing values in the dataset.\n",
        "\n",
        "    Decision trees can handle missing values through surrogate splits\n",
        "    or by treating missing as a separate category.\n",
        "\n",
        "    Parameters:\n",
        "    X: feature matrix with potential missing values\n",
        "    method: str, method for handling missing values\n",
        "\n",
        "    Returns:\n",
        "    X_processed: processed feature matrix\n",
        "    \"\"\"\n",
        "    X_new = X.copy()\n",
        "    for i in range(X_new.shape[1]):\n",
        "        col = X_new[:, i]\n",
        "        missing = np.isnan(col)\n",
        "        if np.any(missing):\n",
        "            if method == 'mean':\n",
        "                col[missing] = np.nanmean(col)\n",
        "            elif method == 'zero':\n",
        "                col[missing] = 0\n",
        "            else:\n",
        "                col[missing] = -1\n",
        "            X_new[:, i] = col\n",
        "    return X_new\n",
        "\n",
        "print(\"Testing advanced features...\")\n",
        "X_val = np.array([[1,2],[2,1],[1,3]])\n",
        "y_val = np.array([0,1,0])\n",
        "tree_root = tree.root\n",
        "\n",
        "tree_root = implement_pruning(tree_root, X_val, y_val)\n",
        "importances = implement_feature_importance(tree_root, X.shape[1])\n",
        "X_processed = implement_missing_value_handling(X)\n",
        "\n",
        "print(\"Feature importances:\", importances)\n",
        "print(\"Processed X:\\n\", X_processed)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}